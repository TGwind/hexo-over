---
title: 数据仓库基础
tags:
- 数据仓库
---
# 数据仓库

## 聚类分析

聚类分析是一种数据分析方法，它将相似的对象分组成为一个簇，使得同一簇内的对象相似度较高，而不同簇之间的对象相似度较低。聚类分析可以用于数据挖掘、图像处理、生物信息学等领域，以发现数据中的模式和结构。聚类分析的目标是将数据集划分为若干个簇，每个簇内的数据点相似度高，而不同簇之间的相似度较低。聚类分析的结果可以用于数据可视化、分类、预测等应用。

聚类分析的分类：

1. 基于分割的聚类：将数据点分配到一些原型中，原型可以是中心点、中心向量或者是概率分布。

- K-Means算法
- K-Medoids算法
- 学习向量量化（LVQ）算法
- 高斯混合模型（GMM）算法

2. 基于层次的聚类：将数据点分配到一些层次结构中，可以是树形结构或者是图形结构。

- 凝聚层次聚类算法
- 分裂层次聚类算法

3. 基于密度的聚类：将数据点分配到一些密度高的区域中。

- DBSCAN算法
- OPTICS算法
- DENCLUE算法

4. 基于网格的聚类：将数据点分配到一些网格中。

- STING算法
- CLIQUE算法

5. 模型聚类：将数据点分配到一些模型中，模型可以是线性或非线性的。

- 谱聚类算法
- 潜在语义分析（LSA）算法
- 非负矩阵分解（NMF）算法

以上是聚类分析的分类及其包含的算法。

## 聚类算法中的数据结构

### 数据矩阵

聚类算法中的数据结构通常是一个数据矩阵，其中每行代表一个样本，每列代表一个特征。这个矩阵可以表示为一个n行m列的矩阵，其中n表示样本数量，m表示特征数量。

### 差异度矩阵

差异度矩阵是聚类分析中的一种数据结构，用于表示不同样本之间的相似度或距离。它是一个方阵，其中每个元素表示两个样本之间的差异度。差异度矩阵可以通过不同的距离度量方法计算得出，例如欧几里得距离、曼哈顿距离、余弦相似度等。

#### 欧几里得距离

假设有三个样本点A、B、C，它们的特征向量分别为$a=(a_1,a_2,\cdots,a_n)$，$b=(b_1,b_2,\cdots,b_n)$，$c=(c_1,c_2,\cdots,c_n)$。则它们之间的欧几里得距离$d_{ij}$可以表示为：

$$d_{ij}=\sqrt{\sum_{k=1}^{n}(x_{ik}-x_{jk})^2}$$

其中，$i,j$表示样本点的编号，$k$表示特征的编号，$x_{ik}$表示第$i$个样本点在第$k$个特征上的取值，$x_{jk}$表示第$j$个样本点在第$k$个特征上的取值。

> 欧几里得距离就是两个样本之间的标准差

### 曼哈顿距离

也称为城市街区距离，是指在规划的城市街区中，两点之间沿着街道走的距离。计算曼哈顿距离的公式如下：

$d(x,y) = \sum_{i=1}^{n}|x_i - y_i|$

其中，$x$和$y$是两个n维向量，$x_i$和$y_i$是向量中第$i$个元素的值。$|x_i - y_i|$表示两个元素之间的差的绝对值。最后将所有差的绝对值相加，得到的就是曼哈顿距离$d(x,y)$。

曼哈顿距离的公式：$d(x,y) = \sum_{i=1}^{n}|x_i - y_i|$

其中，$d(x,y)$表示曼哈顿距离，$x$和$y$表示两个向量，$n$表示向量的维度。

### 举例

假设我们有三个样本点A、B、C，它们的特征向量分别为$a=(1,2,3)$，$b=(4,5,6)$，$c=(7,8,9)$，则它们之间的差异度矩阵为：

|      | A      | B     | C      |
| ---- | ------ | ----- | ------ |
| A    | 0      | 5.196 | 10.392 |
| B    | 5.196  | 0     | 5.196  |
| C    | 10.392 | 5.196 | 0      |

其中，第$i$行第$j$列表示第$i$个样本点和第$j$个样本点之间的欧几里得距离。例如，第一行表示样本点A和其他样本点之间的距离，其中$0$表示A和自己之间的距离为$0$，$5.196$表示A和B之间的距离，$10.392$表示A和C之间的距离。

## K-means算法

K-means算法是一种基于聚类的无监督学习算法，它可以将数据集划分为K个不同的类别。该算法最早由J. MacQueen于1967年提出，后来由Lloyd和Forgy等人进行了改进和推广。K-means算法在数据挖掘、图像处理、模式识别等领域得到了广泛应用。

K-means算法的核心是如何计算聚类中心和数据点之间的距离。常用的距离度量方法有欧几里得距离、曼哈顿距离、切比雪夫距离等。

### 算法伪代码

```
1. 从数据集中随机选择K个点作为初始聚类中心
2. repeat
3.     对于每个数据点，计算它与K个聚类中心的距离，并将其归为距离最近的聚类中心所在的类别
4.     对于每个类别，重新计算它们的聚类中心
5. until 聚类中心不再发生变化或达到最大迭代次数
6. 返回聚类结果
```

## PAM算法

### 背景

PAM算法（Partitioning Around Medoids）是一种聚类算法，它是基于k-medoids算法的改进版本。k-medoids算法是一种基于质心的聚类算法，它将数据点分配到k个簇中，并将每个簇的质心作为该簇的代表点。然而，k-medoids算法的缺点是它对噪声和异常值非常敏感，因为它使用每个簇中的任意数据点作为质心，而不是中心点。

### 算法流程

PAM算法的流程如下：

1. 随机选择k个数据点作为初始的medoids（代表点）。

2. 对于每个数据点，计算它与每个medoid之间的距离，并将该数据点分配到距离最近的medoid所代表的簇中。

3. 对于每个簇，选择一个新的medoid，使得该簇中所有数据点到新的medoid的距离之和最小。

4. 重复步骤2和3，直到簇的分配不再改变或达到最大迭代次数。

5. ### PAM算法伪代码

   ```
   输入：数据集D，聚类数k
   输出：聚类结果C
   
   1. 从数据集D中随机选择k个样本作为初始聚类中心
   2. 初始化聚类结果C，将每个样本分配到距离最近的聚类中心所在的簇中
   3. repeat
   4.     for i in range(k):
   5.         令Ci表示第i个簇，Si表示Ci中的样本集合
   6.         计算Si中每个样本与Ci的其他样本的距离之和，记为W(Ci)
   7.         for j in range(|Si|):
   8.             选择Si中一个非Ci的样本p，将p加入Ci中，得到一个新的簇C'i
   9.             计算C'i中每个样本与C'i的其他样本的距离之和，记为W(C'i)
   10.            计算C'i与Ci的距离，记为d(C'i, Ci)
   11.        end for
   12.        选择使得d(C'i, Ci)最小的C'i作为Ci+1的聚类中心
   13.    end for
   14. until 聚类结果不再发生变化
   15. 返回聚类结果C
   ```

   注：|Si|表示簇Ci中的样本数量。

### 举例

- 假设我们有一个二维数据集，包含5个样本，每个样本有2个特征，如下所示：

  | 样本编号 | 特征1 | 特征2 |
  | -------- | ----- | ----- |
  | 1        | 2     | 3     |
  | 2        | 4     | 5     |
  | 3        | 6     | 7     |
  | 4        | 8     | 9     |
  | 5        | 10    | 11    |

  我们现在要使用PAM算法对这个数据集进行聚类，假设我们要将数据集分成两个簇。

  首先，我们需要随机选择两个样本作为初始簇中心。假设我们选择了样本1和样本3作为初始簇中心。

  接下来，我们需要计算每个样本到这两个簇中心的距离，并将每个样本分配到距离更近的簇中心所在的簇中。计算距离的方法可以使用欧氏距离。计算结果如下所示：

  | 样本编号 | 特征1 | 特征2 | 到簇中心1的距离    | 到簇中心2的距离    | 所属簇 |
  | -------- | ----- | ----- | ------------------ | ------------------ | ------ |
  | 1        | 2     | 3     | 0                  | 5.656854249492381  | 1      |
  | 2        | 4     | 5     | 2.8284271247461903 | 3.1622776601683795 | 1      |
  | 3        | 6     | 7     | 5.656854249492381  | 0                  | 2      |
  | 4        | 8     | 9     | 11.313708498984761 | 5.656854249492381  | 2      |
  | 5        | 10    | 11    | 16.970562748477143 | 11.313708498984761 | 2      |

  接下来，我们需要计算每个簇中所有样本到该簇中心的距离之和，作为该簇的代价。计算结果如下所示：

  | 簇编号 | 簇中心 | 样本编号 | 特征1 | 特征2 | 到簇中心的距离     | 代价 |
  | ------ | ------ | -------- | ----- | ----- | ------------------ | ---- |
  | 1      | 2, 4   | 1        | 2     | 3     | 0                  | 0    |
  | 1      | 2, 4   | 2        | 4     | 5     | 2.8284271247461903 | 8    |
  | 2      | 6, 7   | 3        | 6     | 7     | 0                  | 0    |
  | 2      | 6, 7   | 4        | 8     | 9     | 5.656854249492381  | 32   |
  | 2      | 6, 7   | 5        | 10    | 11    | 11.313708498984761 | 72   |

  接下来，我们需要选择一个样本，将其替换为另一个样本，以尽可能降低代价。具体来说，我们需要选择一个非簇中心的样本，将其替换为另一个非簇中心的样本，使得替换后的代价最小。假设我们选择将样本2替换为样本4，计算替换后的代价如下所示：

  | 簇编号 | 簇中心 | 样本编号 | 特征1 | 特征2 | 到簇中心的距离     | 代价 |
  | ------ | ------ | -------- | ----- | ----- | ------------------ | ---- |
  | 1      | 2, 4   | 1        | 2     | 3     | 0                  | 0    |
  | 1      | 2, 4   | 4        | 8     | 9     | 5.656854249492381  | 18   |
  | 2      | 6, 7   | 3        | 6     | 7     | 0                  | 0    |
  | 2      | 6, 7   | 5        | 10    | 11    | 11.313708498984761 | 72   |

  可以看到，替换后的代价为18，比原来的代价32要小。因此，我们将样本2替换为样本4，得到新的簇划分如下所示：

  | 样本编号 | 特征1 | 特征2 | 到簇中心1的距离    | 到簇中心2的距离    | 所属簇 |
  | -------- | ----- | ----- | ------------------ | ------------------ | ------ |
  | 1        | 2     | 3     | 0                  | 5.656854249492381  | 1      |
  | 2        | 8     | 9     | 5.656854249492381  | 11.313708498984761 | 2      |
  | 3        | 6     | 7     | 5.656854249492381  | 0                  | 2      |
  | 4        | 8     | 9     | 11.313708498984761 | 5.656854249492381  | 1      |
  | 5        | 10    | 11    | 16.970562748477143 | 11.313708498984761 | 2      |

  接下来，我们需要重新计算每个簇的中心，并重复以上步骤，直到簇划分不再改变为止。

### 与k-means算法区别

PAM算法与k-means算法的区别在于，PAM算法使用medoids作为代表点，而k-means算法使用质心作为代表点。因此，PAM算法对噪声和异常值的容忍度更高，因为medoids是实际存在于数据集中的数据点，而质心可能不是。

### 优劣

PAM算法的优点是它对噪声和异常值的容忍度更高，因为它使用实际存在于数据集中的数据点作为代表点。此外，PAM算法可以处理非欧几里德距离，因为它只需要计算数据点之间的距离即可。

PAM算法的缺点是它的计算复杂度较高，因为它需要计算每个数据点与每个medoid之间的距离，并且需要选择新的medoid。此外，PAM算法对于大型数据集的处理效率较低。

# 分类算法

有许多不同的分类算法可供选择，以下是一些常见的分类算法：

1. 逻辑回归（Logistic Regression）：逻辑回归是一种基本的二分类算法，通过使用逻辑函数将输入特征映射到概率输出。

2. 决策树（Decision Trees）：决策树使用树状结构来进行决策，通过对特征进行逐步划分来进行分类。

3. 随机森林（Random Forests）：随机森林是一种集成学习方法，它由多个决策树组成，并通过投票或平均预测来进行分类。

4. 支持向量机（Support Vector Machines，SVM）：SVM 是一种二分类算法，它通过寻找一个最优的超平面来最大化类别间的间隔。

5. 朴素贝叶斯（Naive Bayes）：朴素贝叶斯是一种基于贝叶斯定理的概率分类算法，它假设特征之间是相互独立的。

6. K最近邻算法（K-Nearest Neighbors，KNN）：KNN 根据特征空间中的最近邻样本的标签进行分类，其中 K 是一个用户指定的参数，表示最近邻样本的数量。

7. 神经网络（Neural Networks）：神经网络是一种基于人工神经元模型的机器学习模型，可以用于解决分类问题。

8. 梯度提升树（Gradient Boosting Trees）：梯度提升树通过逐步迭代地训练多个弱分类器，并加权组合它们的预测结果来进行分类。

这些只是分类算法的一部分，还有其他许多算法和变体可供选择，具体选择哪个算法取决于数据集的特点、问题的性质以及个人偏好。

## 决策树

决策树是一种基于树形结构的分类和回归模型。它通过对数据集进行划分，构建一棵树来进行决策。每个节点代表一个特征，每个分支代表该特征的一个取值，每个叶子节点代表一个分类或回归结果。在分类问题中，决策树可以用于判断一个样本属于哪个类别；在回归问题中，决策树可以用于预测一个连续值。决策树具有易于理解、易于解释、易于实现等优点，因此被广泛应用于数据挖掘、机器学习等领域。

生成决策树的一般步骤如下：

1. 收集数据：收集数据是生成决策树的第一步，需要收集有关决策问题的所有相关数据。

2. 准备数据：准备数据是生成决策树的第二步，需要将收集到的数据进行处理，包括数据清洗、数据转换、数据归一化等。

3. 选择属性：选择属性是生成决策树的第三步，需要根据数据集中的属性选择最佳的属性作为划分标准。

4. 划分数据集：划分数据集是生成决策树的第四步，需要将数据集根据选择的属性进行划分，得到子集。

5. 计算信息增益：计算信息增益是生成决策树的第五步，需要计算每个子集的信息熵，以及划分前后的信息增益。

6. 递归生成决策树：递归生成决策树是生成决策树的最后一步，需要根据计算得到的信息增益，选择最佳的属性作为当前节点，然后递归生成子树，直到所有的叶子节点都是同一类别。

7. 剪枝：剪枝是生成决策树的最后一步，需要对生成的决策树进行剪枝，以避免过拟合的问题。

### ID3算法

ID3算法是一种用于决策树分类的算法，它基于信息熵的概念，通过计算每个特征的信息增益来选择最优特征，从而构建决策树。

ID3算法的步骤如下：

1. 计算数据集的熵（entropy），公式为：$H(D)=-\sum_{i=1}^{n}p_i\log_2p_i$，其中$n$为类别数，$p_i$为第$i$类样本在数据集中的占比。
2. 对于每个特征，计算其信息增益（information gain），公式为：$IG(D, f)=H(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}H(D^v)$，其中$f$为特征，$V$为特征$f$的取值个数，$D^v$为特征$f$取值为$v$的样本子集，$|D^v|$为$D^v$的样本个数，$|D|$是数据集样本个数。
3. 选择信息增益最大的特征作为当前节点的划分特征，将数据集划分为若干个子集，每个子集对应一个分支。
4. 对于每个子集，如果所有样本属于同一类别，则将该子集作为叶节点，否则递归地进行步骤2-3，直到所有子集都被划分为同一类别或者没有更多特征可供划分。

ID3算法的缺点包括：

1. 对于连续型变量的处理不够灵活。ID3算法只能处理离散型变量，对于连续型变量需要进行离散化处理，这可能会导致信息损失。

2. 对于噪声数据和缺失数据的处理不够鲁棒。ID3算法对于噪声数据和缺失数据的处理能力较弱，可能会导致决策树的不稳定性。

3. 容易出现过拟合现象。ID3算法在构建决策树时，容易出现过拟合现象，即对训练数据过度拟合，导致泛化能力较差。

4. 对于类别数较多的数据集，决策树的构建效率较低。ID3算法在处理类别数较多的数据集时，需要进行大量的计算，导致构建效率较低。

这些缺点主要是由于ID3算法的基本思想是贪心算法，只考虑当前节点的最优划分，而没有考虑全局最优解。因此，在实际应用中，需要根据具体情况选择合适的决策树算法。

下面是一个简单的例子，假设我们有一个数据集，其中包含5个样本，每个样本有两个特征（色泽和根蒂），以及一个类别（好瓜或坏瓜）：

| 色泽 | 根蒂 | 类别 |
| ---- | ---- | ---- |
| 青绿 | 蜷缩 | 好瓜 |
| 乌黑 | 蜷缩 | 好瓜 |
| 乌黑 | 硬挺 | 坏瓜 |
| 青绿 | 硬挺 | 坏瓜 |
| 浅白 | 蜷缩 | 坏瓜 |

首先，我们需要计算数据集的熵：

$H(D)=-\frac{3}{5}\log_2\frac{3}{5}-\frac{2}{5}\log_2\frac{2}{5}=0.971$

然后，对于每个特征，我们需要计算其信息增益：

- 对于色泽特征：（浅白时特征的信息熵为0，所以省略）

$IG(D, 色泽)=0.971-\frac{3}{5}(-\frac{2}{3}\log_2\frac{2}{3}-\frac{1}{3}\log_2\frac{1}{3})-\frac{2}{5}(-\frac{1}{2}\log_2\frac{1}{2}-\frac{1}{2}\log_2\frac{1}{2})=0.019$

- 对于根蒂特征：

$IG(D, 根蒂)=0.971-\frac{3}{5}(-\frac{1}{3}\log_2\frac{1}{3}-\frac{2}{3}\log_2\frac{2}{3})-\frac{2}{5}(-\frac{1}{2}\log_2\frac{1}{2}-\frac{1}{2}\log_2\frac{1}{2})=0.271$

因此，根蒂特征的信息增益最大，我们选择根蒂特征作为当前节点的划分特征，将数据集划分为两个子集：

- 根蒂为蜷缩的子集：{青绿, 蜷缩, 好瓜}和{乌黑, 蜷缩, 好瓜}
- 根蒂为硬挺的子集：{乌黑, 硬挺, 坏瓜}和{青绿, 硬挺, 坏瓜}和{浅白, 蜷缩, 坏瓜}

对于根蒂为蜷缩的子集，所有样本属于同一类别（好瓜），因此将该子集作为叶节点；对于根蒂为硬挺的子集，我们需要递归地进行步骤2-3，选择信息增益最大的特征进行划分。在这个例子中，我们可以选择色泽特征作为下一个划分特征，因为它是唯一一个还没有被使用过的特征。最终得到的决策树如下：

```
根蒂
├── 蜷缩: 好瓜
└── 硬挺
    ├── 青绿
    │   └── 色泽
    │       ├── 浅白: 坏瓜
    │       └── 青绿: 坏瓜
    └── 乌黑: 坏瓜
```

这个决策树可以用于预测新的样本的类别。例如，如果我们有一个新的样本，它的色泽为乌黑，根蒂为蜷缩，我们可以按照决策树的路径进行判断，最终得到它的类别为好瓜。

以下是一个简单的Python实现ID3算法的示例代码：

```python
import math

def entropy(data):
    """
    计算数据集的熵
    """
    count = {}
    for d in data:
        label = d[-1]
        if label not in count:
            count[label] = 0
        count[label] += 1
    entropy = 0
    for c in count.values():
        p = c / len(data)
        entropy -= p * math.log2(p)
    return entropy

def split_data(data, axis, value):
    """
    根据给定特征和特征值划分数据集
    """
    new_data = []
    for d in data:
        if d[axis] == value:
            new_d = d[:axis] + d[axis+1:]
            new_data.append(new_d)
    return new_data

def choose_feature(data):
    """
    选择最优特征
    """
    num_features = len(data[0]) - 1
    base_entropy = entropy(data)
    best_info_gain = 0
    best_feature = -1
    for i in range(num_features):
        feature_values = [d[i] for d in data]
        unique_values = set(feature_values)
        new_entropy = 0
        for value in unique_values:
            sub_data = split_data(data, i, value)
            p = len(sub_data) / len(data)
            new_entropy += p * entropy(sub_data)
        info_gain = base_entropy - new_entropy
        if info_gain > best_info_gain:
            best_info_gain = info_gain
            best_feature = i
    return best_feature

def majority_label(labels):
    """
    返回出现次数最多的标签
    """
    count = {}
    for label in labels:
        if label not in count:
            count[label] = 0
        count[label] += 1
    sorted_count = sorted(count.items(), key=lambda x: x[1], reverse=True)
    return sorted_count[0][0]

def create_tree(data, features):
    """
    创建决策树
    """
    labels = [d[-1] for d in data]
    if len(set(labels)) == 1:
        return labels[0]
    if len(data[0]) == 1:
        return majority_label(labels)
    best_feature = choose_feature(data)
    best_feature_name = features[best_feature]
    tree = {best_feature_name: {}}
    del(features[best_feature])
    feature_values = [d[best_feature] for d in data]
    unique_values = set(feature_values)
    for value in unique_values:
        sub_features = features[:]
        sub_data = split_data(data, best_feature, value)
        tree[best_feature_name][value] = create_tree(sub_data, sub_features)
    return tree

if __name__ == '__main__':
    data = [
        [1, 1, 'yes'],
        [1, 1, 'yes'],
        [1, 0, 'no'],
        [0, 1, 'no'],
        [0, 1, 'no'],
    ]
    features = ['no surfacing', 'flippers']
    tree = create_tree(data, features)
    print(tree)
```

输出结果为：

```sh
{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}
```

这个决策树可以解释为：

- 如果没有浮出水面，则不会吸氧，预测结果为“不会游泳”；
- 如果浮出水面，则可能会吸氧，需要进一步判断是否有鳍；
- 如果有鳍，则预测结果为“会游泳”；
- 如果没有鳍，则预测结果为“不会游泳”。

### C4.5算法

C4.5算法是一种决策树算法，是ID3算法的改进版本。C4.5算法在ID3算法的基础上进行了优化，主要包括以下几个方面：

1. C4.5算法能够处理连续型属性，而ID3算法只能处理离散型属性。

2. C4.5算法采用信息增益比来选择最优划分属性，而ID3算法只采用信息增益。

3. C4.5算法能够处理缺失值，而ID3算法不能处理缺失值。

4. C4.5算法采用剪枝技术来避免过拟合，而ID3算法没有剪枝技术。

C4.5算法的主要思想是通过递归地将数据集划分为更小的子集，直到所有子集都属于同一类别或者达到预定的停止条件。在划分数据集的过程中，C4.5算法采用信息增益比来选择最优划分属性，同时使用剪枝技术来避免过拟合。

C4.5算法是一种决策树算法，是ID3算法的改进版。C4.5算法在ID3算法的基础上，解决了ID3算法的一些问题，如处理连续属性、处理缺失值、剪枝等。C4.5算法的核心思想是利用信息增益比来选择最优划分属性。

C4.5算法的具体步骤如下：

1. 构建决策树的根节点，将所有训练样本放入根节点。

2. 对于每个非叶子节点，计算每个属性的信息增益比，选择信息增益比最大的属性作为划分属性。

3. 根据划分属性将训练样本分成若干个子集，每个子集对应一个子节点。

4. 对于每个子节点，如果所有训练样本都属于同一类别，则将该节点标记为叶子节点，并将该类别作为叶子节点的类别；否则，递归地对该子节点进行步骤2-4，直到所有叶子节点都被标记。

5. 对生成的决策树进行剪枝，避免过拟合。

C4.5算法中用到的公式如下：

1. 信息熵（Entropy）：

$$H(X)=-\sum_{i=1}^{n}p_i\log_2p_i$$

其中，$X$为一个随机变量，$n$为$X$的取值个数，$p_i$为$X$取值为$i$的概率。

2. 条件熵（Conditional Entropy）：

$$H(Y|X)=\sum_{i=1}^{m}p_iH(Y|X=x_i)$$

其中，$X$为一个随机变量，$m$为$X$的取值个数，$p_i$为$X$取值为$i$的概率，$Y$为另一个随机变量，$H(Y|X=x_i)$为在$X=x_i$的条件下$Y$的熵。

3. 信息增益（Information Gain）：

$$IG(Y,X)=H(Y)-H(Y|X)$$

其中，$X$为一个属性，$Y$为类别标签。

4. 信息增益比（Information Gain Ratio）：

$$GR(Y,X)=\frac{IG(Y,X)}{H(X)}$$

其中，$X$为一个属性，$Y$为类别标签。



C4.5算法对于连续型属性的离散化处理主要有以下几个步骤：

1. 对于每个连续型属性，将其按照从小到大的顺序排序。

2. 对于每个相邻的属性值，计算其中点值，作为一个候选的分割点。

3. 对于每个候选的分割点，将数据集按照该点进行分割，并计算分割后的信息增益。

4. 选择信息增益最大的分割点作为最终的分割点。

5. 将数据集按照最终的分割点进行分割，并将连续型属性转化为离散型属性，分别表示小于等于分割点和大于分割点两个取值。



### 预剪枝

### 后剪枝



## 支持向量机


---
title: 大数据存储技术

---
## 高可用性

高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间

## 高性能

软件的高性能是指软件响应及时度满足用户要求的程度很高。
狭义的讲，软件高性能是指软件在尽可能少地占用系统资源的前提下，尽可能的提高运行速度；广义的讲，软件高性能指[软件质量](https://so.csdn.net/so/search?q=软件质量&spm=1001.2101.3001.7020)高，包括正确性、可靠性、易用性、安全性、可扩展性、兼容性和可移植性

## 数据一致性

在分布式系统中我们为了提高系统的可用性，也是不可避免的使用副本的机制，引入了副本则就需要同步数据到不同的副本，从而引发了副本一致性的问题。分布式系统独立部署且分别使用不同的数据库，每个服务内部又是使用数据库集群，数据在服务与服务之间、在某个服务的数据库集群中间等等的流转、同步，这些过程都是有网络、时间消耗的，一个数据从最开始的产生到它应该到的地方不会瞬时完成

数据库一致性（Database Consistency）是指事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。保证数据库一致性是指当事务完成时，必须使所有数据都具有一致的状态。

## CAP理论

一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项

CAP原则又称CAP定理，指的是在一个[分布式系统](https://so.csdn.net/so/search?q=分布式系统&spm=1001.2101.3001.7020)中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾

一致性(Consistency) (所有节点在同一时间具有相同的数据)

可用性(Availability) (保证每个请求不管成功或者失败都有响应)

分区容错性(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作)

## Raft算法

raft是一个共识算法（consensus algorithm），所谓共识，就是多个节点对某个事情达成一致的看法，即使是在部分节点故障、网络延时、网络分割的情况下。这些年最为火热的加密货币（比特币、区块链）就需要共识算法，而在分布式系统中，共识算法更多用于提高系统的容错性，比如分布式存储中的复制集（replication），在[带着问题学习分布式系统之中心化复制集](https://www.cnblogs.com/xybaby/p/7153755.html)一文中介绍了中心化复制集的相关知识。raft协议就是一种leader-based的共识算法，与之相应的是leaderless的共识算法。

#### **Raft 角色**

根据官方文档解释，一个 Raft 集群包含若干节点，Raft 把这些节点分为三种状态：Leader、 Follower、Candidate，每种状态负责的任务也是不一样的。正常情况下，集群中的节点只存在 Leader 与 Follower 两种状态。

• **Leader（领导者）** ：负责日志的同步管理，处理来自客户端的请求，与Follower保持heartBeat的联系；

• **Follower（追随者）** ：响应 Leader 的日志同步请求，响应Candidate的邀票请求，以及把客户端请求到Follower的事务转发（重定向）给Leader；

• **Candidate（候选者）** ：负责选举投票，集群刚启动或者Leader宕机时，状态为Follower的节点将转为Candidate并发起选举，选举胜出（获得超过半数节点的投票）后，从Candidate转为Leader状态。

#### **Raft 三个子问题**

通常，Raft 集群中只有一个 Leader，其它节点都是 Follower。Follower 都是被动的，不会发送任何请求，只是简单地响应来自 Leader 或者 Candidate 的请求。Leader 负责处理所有的客户端请求（如果一个客户端和 Follower 联系，那么 Follower 会把请求重定向给 Leader）。

为简化逻辑和实现，Raft 将一致性问题分解成了三个相对独立的子问题。

• **选举（Leader Election）** ：当 Leader 宕机或者集群初创时，一个新的 Leader 需要被选举出来；

• **日志复制（Log Replication）** ：Leader 接收来自客户端的请求并将其以日志条目的形式复制到集群中的其它节点，并且强制要求其它节点的日志和自己保持一致；

• **安全性（Safety）** ：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其它服务器节点不能在同一个日志索引位置应用一个不同的指令。

### **Raft 算法之 Leader Election 原理**

根据 Raft 协议，一个应用 Raft 协议的集群在刚启动时，所有节点的状态都是 Follower。由于没有 Leader，Followers 无法与 Leader 保持心跳（Heart Beat），因此，Followers 会认为 Leader 已经下线，进而转为 Candidate 状态。然后，Candidate 将向集群中其它节点请求投票，同意自己升级为 Leader。如果 Candidate 收到超过半数节点的投票（N/2 + 1），它将获胜成为 Leader。

## Nginx的负载均衡策略

[Nginx的负载均衡](https://cloud.tencent.com/developer/article/1774118)

Nginx 是一款开源的高性能轻量级 Web 服务器（也叫 HTTP 服务器），它主要提供的功能是：反向代理、负载均衡和HTTP 缓存。它于 2004 年首次公开发布，2011 年成立同名公司以提供支持，2019 年 3 月被 F5 Networks 以 6.7 亿美元收购。

在服务器集群中，Nginx起到一个代理服务器的角色（即反向代理），为了避免单独一个服务器压力过大，将来自用户的请求转发给不同的服务器。

之所以需要使用负载均衡是因为，如果我们使用的是一台服务器，那么在高峰期时很多用户就需要排队等待系统响应，因为一台服务器能处理的并发数是固定的。例如，一个 Tomcat 在默认情况下只能开启 150 个线程（Tomcat 8.5.x 版本）来处理并发任务，如果并发数超过了最大线程数，那么新来的请求就只能排队等待处理了，如下图所示：

![img](http://myblog.over2022.top/e70y3e7vl7.webp) 

然而如果有负载均衡的话，我们就可以将所有的请求分配到不同的服务器上。假如 1 台服务器可以处理 2000 个请求，那么 5 台服务器就可以处理 10000 个请求了，这样就大大提高了系统处理业务的能力，如下图所示：

![img](http://myblog.over2022.top/zqfigamt40.webp) 

Nginx 主要的负载均衡策略（内置的负载均衡）有以下四种：

- **轮询策略（默认负载均衡策略）**
- **最少连接数负载均衡策略**
- **ip-hash 负载均衡策略**
- **权重负载均衡策略**

### 轮循策略

轮询负载策略是指每次将请求按顺序轮流发送至相应的服务器上，它的配置示例如下所示：

![img](http://myblog.over2022.top/ex34fb9lcj.webp) 

在以上实例中，当我们使用“ip:80/”访问时，请求就会轮询的发送至上面配置的三台服务器上。

Nginx 可以实现 HTTP、HTTPS、FastCGI、uwsgi、SCGI、memcached 和 gRPC 的负载均衡。

### 最少连接数负载均衡策略

此策略是指每次将请求分发到当前连接数最少的服务器上，也就是 Nginx 会将请求试图转发给相对空闲的服务器以实现负载平衡，它的配置示例如下：

![img](http://myblog.over2022.top/y3yuxb7x8l.webp)

### ip-hash 负载均衡策略

以上三种负载均衡的配置策略都不能保证将每个客户端的请求固定的分配到一台服务器上。假如用户的登录信息是保存在单台服务器上的，而不是保存在类似于 Redis 这样的第三方中间件上时，如果不能将每个客户端的请求固定的分配到一台服务器上，就会导致用户的登录信息丢失。因此用户在每次请求服务器时都需要进行登录验证，这样显然是不合理的，也是不能被用户所接受的，所以在特殊情况下我们就需要使用 ip-hash 的负载均衡策略。

ip-hash 负载均衡策略可以根据客户端的 IP，将其固定的分配到相应的服务器上，它的配置示例如下：

![img](http://myblog.over2022.top/xorhaohbnp.webp)

Nginx 的实现原理是，首先客户端通过访问[域名](https://cloud.tencent.com/act/pro/domain-sales?from=10680)地址发出 HTTP 请求，访问的域名会被 DNS 服务器解析为 Nginx 的 IP 地址，然后将请求转发至 Nginx 服务器，Nginx 接收到请求之后会通过 URL 地址和负载均衡的配置，匹配到配置的代理服务器，然后将请求转发给代理服务器，代理服务器拿到请求之后将处理结果返回给 Nginx，Nginx 再将结果返回给客户端，这样就完成了一次正常的 HTTP 交互。

### 加权负载均衡

此配置方式是指每次会按照服务器配置的权重进行请求分发，**权重高的服务器会收到更多的请求**，这就相当于给 Nginx 在请求分发时加了一个参考的权重选项，并且这个权重值是可以人工配置的。因此我们就可以将硬件配置高，以及并发能力强的服务器的权重设置高一点，以更合理地利用服务器的资源，它配置示例如下：

![img](http://myblog.over2022.top/ium8phniv8.webp)

以上配置表示，5 次请求中有 3 次请求会分发给 srv1，1 次请求会分发给 srv2，另外 1 次请求会分发给 srv3。

Nginx 的四种内置负载均衡的执行策略：轮询策略（默认负载均衡策略）、最少连接数负载均衡策略、ip-hash 负载均衡策略和权重负载均衡策略，其中 ip-hash 的负载均衡策略会将客户端的请求固定分发到一台服务器上。

## 一致性hash算法

[一致性hash算法](https://cloud.tencent.com/developer/article/1955794#:~:text=%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%E5%9C%A8%201997%20%E5%B9%B4%E7%94%B1%E9%BA%BB%E7%9C%81%E7%90%86%E5%B7%A5%E5%AD%A6%E9%99%A2%E6%8F%90%E5%87%BA%EF%BC%8C%E6%98%AF%E4%B8%80%E7%A7%8D%E7%89%B9%E6%AE%8A%E7%9A%84%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%EF%BC%8C%E5%9C%A8%E7%A7%BB%E9%99%A4%E6%88%96%E8%80%85%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AA%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%B6%EF%BC%8C%E8%83%BD%E5%A4%9F%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%B0%8F%E5%9C%B0%E6%94%B9%E5%8F%98%E5%B7%B2%E5%AD%98%E5%9C%A8%E7%9A%84%E6%9C%8D%E5%8A%A1%E8%AF%B7%E6%B1%82%E4%B8%8E%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E6%98%A0%E5%B0%84%E5%85%B3%E7%B3%BB%EF%BC%9B%20%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E8%A7%A3%E5%86%B3%E4%BA%86%E7%AE%80%E5%8D%95%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%E5%9C%A8%E5%88%86%E5%B8%83%E5%BC%8F,%E5%93%88%E5%B8%8C%E8%A1%A8%20%EF%BC%88Distributed%20Hash%20Table%EF%BC%8CDHT%EF%BC%89%E4%B8%AD%E5%AD%98%E5%9C%A8%E7%9A%84%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E7%AD%89%E9%97%AE%E9%A2%98%EF%BC%9B)

### **一致性 Hash 算法背景**

考虑这么一种场景：

我们有三台缓存[服务器](https://cloud.tencent.com/product/cvm?from=10680)编号`node0`、`node1`、`node2`，现在有 3000 万个`key`，希望可以将这些个 key 均匀的缓存到三台机器上，你会想到什么方案呢？

我们可能首先想到的方案是：取模算法`hash（key）% N`，即：对 key 进行 hash 运算后取模，N 是机器的数量；

这样，对 key 进行 hash 后的结果对 3 取模，得到的结果一定是 0、1 或者 2，正好对应服务器`node0`、`node1`、`node2`，存取数据直接找对应的服务器即可，简单粗暴，完全可以解决上述的问题；

![img](http://myblog.over2022.top/abec5d480903d8b2e6a5948170411ea1.jpeg)

取模算法虽然使用简单，但对机器数量取模，在集群扩容和收缩时却有一定的局限性：**因为在生产环境中根据业务量的大小，调整服务器数量是常有的事；**

**而服务器数量 N 发生变化后`hash（key）% N`计算的结果也会随之变化！**

![img](http://myblog.over2022.top/a4c17914243424a78e1298431d3445b3.jpeg)

**比如：一个服务器节点挂了，计算公式从`hash（key）% 3`变成了`hash（key）% 2`，结果会发生变化，此时想要访问一个 key，这个 key 的缓存位置大概率会发生改变，那么之前缓存 key 的数据也会失去作用与意义；**

**大量缓存在同一时间失效，造成缓存的雪崩，进而导致整个缓存系统的不可用，这基本上是不能接受的；**

为了解决优化上述情况，一致性 hash 算法应运而生~

#### **算法原理**

> 一致性哈希算法在 1997 年由麻省理工学院提出，是一种特殊的哈希算法，在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系； 一致性哈希解决了简单哈希算法在分布式[**哈希表**](https://link.segmentfault.com/?enc=8SLNH%2BJkz1wSUKDQoMpUHQ%3D%3D.d6lGfewjMMLemIFvUa01RtDhzMFVr3f3KDOee9wh%2BKofqOsfRFAUjbNFSu8hZ6mQSLJllSWS62WHskJ0a0tkhdU4zmtmnEgyaQYtUxL2FDE%3D)（Distributed Hash Table，DHT）中存在的动态伸缩等问题；

一致性 hash 算法本质上也是一种取模算法；

不过，不同于上边按服务器数量取模，一致性 hash 是**对固定值 2^32 取模**；

> **IPv4 的地址是 4 组 8 位 2 进制数组成，所以用 2^32 可以保证每个 IP 地址会有唯一的映射；**

##### **① hash 环**

我们可以将这`2^32`个值抽象成一个圆环 ⭕️，圆环的正上方的点代表 0，顺时针排列，以此类推：1、2、3…直到`2^32-1`，而这个由 2 的 32 次方个点组成的圆环统称为`hash环`；

![img](http://myblog.over2022.top/fc48f539bd957e74ef54c2e71c3052f1.jpeg)

##### **② 服务器映射到 hash 环**

在对服务器进行映射时，使用`hash（服务器ip）% 2^32`，即：

**使用服务器 IP 地址进行 hash 计算，用哈希后的结果对`2^32`取模，结果一定是一个 0 到`2^32-1`之间的整数；**

**而这个整数映射在 hash 环上的位置代表了一个服务器，依次将`node0`、`node1`、`node2`三个缓存服务器映射到 hash 环上；**

![img](http://myblog.over2022.top/16be733928f3f852df11fd383ee16c59.jpeg)

##### **③ 对象 key 映射到服务器**

在对对应的 Key 映射到具体的服务器时，需要首先计算 Key 的 Hash 值：`hash（key）% 2^32`；

> **注：此处的 Hash 函数可以和之前计算服务器映射至 Hash 环的函数不同，只要保证取值范围和 Hash 环的范围相同即可（即：`2^32`）；**

将 Key 映射至服务器遵循下面的逻辑：

**从缓存对象 key 的位置开始，沿顺时针方向遇到的第一个服务器，便是当前对象将要缓存到的服务器；**

以上便是一致性 Hash 的工作原理；

> **可以看到，一致性 Hash 就是：将原本单个点的 Hash 映射，转变为了在一个环上的某个片段上的映射！**



## 全局ID生成

总的来说，大概有三大类方法，分别是：数据库自增ID、UUID生成、snowflake雪花算法

**一、数据库自增ID**

核心思想：使用数据库的id自增策略（如: Mysql的auto_increment）。

优点：
① 简单，天然有序。

缺点：
① 并发性不好。
② 数据库写压力大。
③ 数据库故障后不可使用。
④ 存在数量泄露风险。

**二、UUID生成**

核心思想：结合机器的网卡（基于名字空间/名字的散列值MD5/SHA1）、当地时间（基于时间戳&时钟序列）、一个随记数来生成UUID。

其结构如下：
aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee（即包含32个16进制数字，以连字号-分为五段，最终形成“8-4-4-4-12”的36个字符的字符串，即32个英数字母+4个连字号）。
例如：550e8400-e29b-41d4-a716-446655440000

优点：
① 本地生成，没有网络消耗，生成简单，没有高可用风险。

缺点：
① 不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。
② 信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。
③ 无序查询效率低：由于生成的UUID是无序不可读的字符串，所以其查询效率低。

**三、雪花算法**

核心思想：把64-bit分别划分成多段，分开来标示机器、时间、某一并发序列等，从而使每台机器及同一机器生成的ID都是互不相同。

PS：这种结构是雪花算法提出者Twitter的分法，但实际上这种算法使用可以很灵活，根据自身业务的并发情况、机器分布、使用年限等，可以自由地重新决定各部分的位数，从而增加或减少某部分的量级。比如：百度的UidGenerator、美团的Leaf等，都是基于雪花算法做一些适合自身业务的变化。

## 设计一个缓存框架

从设计缓存框架的角度来看，应该考虑四个主要方面：**选择合适的缓存存储机制，管理缓存空间，实现缓存失效机制和缓存安全策略**。

选择合适的缓存存储机制：在设计缓存框架时，重要的一步是选择合适的缓存存储机制，可以使用分布式、内存或者文件系统等不同的存储方式。

管理缓存空间：管理缓存空间，可以采用LRU（最近最久未使用）算法、FIFO（先进先出）算法来进行空间管理，以最大程度地利用有限的空间。

实现缓存失效机制：针对缓存中某些不常使用的数据，需要实现失效机制，以清除这些无效数据，如果失效机制不好，可能会导致缓存空间被过度占用，影响缓存框架的效率。缓存安全策略：缓存的安全性也是设计缓存框架时需要考虑的，比如限制缓存的访问权限，确保缓存不被恶意篡改等

## 缓存击穿、穿透、雪崩

**缓存穿透**是指缓存和数据库中都没有的数据，而用户不断发起请求，会导致数据库压力过大。

解决方案：

1. 接口层增加校验，如用户鉴权校验，id做基础校验，id<=0的直接拦截；
2. 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击

[缓存击穿](https://so.csdn.net/so/search?q=缓存击穿&spm=1001.2101.3001.7020)是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力

解决方案：

采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。

1. 设置热点数据永远不过期。
2. 加互斥锁

**缓存雪崩**是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机。和缓存击穿不同的是，        缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。

解决方案：

1. 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。
2. 如果缓存数据库是分布式部署，将热点数据均匀分布在不同搞得缓存数据库中
3. 设置热点数据永远不过期。

## Ehcache的注解

其实EhCache使用的就是Spring Cache的注解，需要注意的是当一个支持缓存的方法在对象内部被调用时是不会触发缓存功能的。
@Cacheable参数：value指被缓存在哪个Cache上，key指缓存方法的返回结果对应的key，condition指什么情况下缓存。
@Cacheable如果在缓存中发现相同的key则会从缓存中取数据。
@CachePut每次都会重新执行，然后将结果放入缓存。
@CacheEvict表示清除缓存。
如果没有指定key。这是用Key的默认策略是（使用参数），也可以自定义key的生成策略。

## Memcacehed的内存管理机制

Memcached内存管理机制是一个**基于分配与回收的机制**。它通过分配一块固定大小的内存池来实现内存管理，当需要存储数据时，它会从内存池中分配一块足够大的空间来存储数据;当数据不再需要使用时，内存将被回收重新分配给其他数据。

Memcached内存管理机制采用了**LRU(Least Recently Used,最近最少使用)淘汰算法**。当内存池已满，并且需要存储新数据时，**LRU淘汰算法会将最近最少使用的数据淘汰，以便为新数据腾出空间**。这种机制确保了Memcached总是有足够的内存空间来存储当前需要的数据，同时避免了内存浪费的情况。



## 关系型数据库与非关系型数据库

### 关系型数据库的优势：

1.  采用二维表结构非常贴近正常开发逻辑（关系型数据模型相对层次型数据模型和网状型数据模型等其他模型来说更容易理解）； 
2.  支持通用的SQL（结构化查询语言）语句； 
3.  丰富的完整性大大减少了数据冗余和数据不一致的问题。并且全部由表结构组成，文件格式一致； 
4.  可以用SQL句子多个表之间做非常繁杂的查询； 
5.  关系型数据库提供对事务的支持，能保证系统中事务的正确执行，同时提供事务的恢复、回滚、并发控制和死锁问题的解决。 
6.  [数据存储](https://cloud.tencent.com/product/cdcs?from=10680)在磁盘中，安全可靠。

### 关系型数据库存在的不足:

>  **随着互联网企业的不断发展，数据日益增多，因此关系型数据库面对海量的数据会存在很多的不足。** 

1.  高并发读写能力差：网站类用户的并发性访问非常高，而一台数据库的最大连接数有限，且硬盘 I/O 有限，不能满足很多人同时连接。 
2.  海量数据情况下读写效率低：对大数据量的表进行读写操作时，需要等待较长的时间等待响应。 
3.  可扩展性不足：不像web server和app server那样简单的添加硬件和服务节点来拓展性能和负荷工作能力。 
4.  数据模型灵活度低：关系型数据库的数据模型定义严格，无法快速容纳新的数据类型（需要提前知道需要存储什么样类型的数据）。

### 非关系型数据库的优势：

1.  非关系型数据库存储数据的格式可以是 key-value 形式、文档形式、图片形式等。使用灵活，应用场景广泛，而关系型数据库则只支持基础类型。 
2.  速度快，效率高。 NoSQL 可以使用硬盘或者随机存储器作为载体，而关系型数据库只能使用硬盘。 
3.  海量数据的维护和处理非常轻松，成本低。 
4.  非关系型数据库具有扩展简单、高并发、高稳定性、成本低廉的优势。 
5.  可以实现数据的分布式处理。

### 非关系型数据库存在的不足:

1.  非关系型数据库暂时不提供 SQL 支持，学习和使用成本较高。 
2.  非关系数据库没有事务处理，无法保证数据的完整性和安全性。适合处理海量数据，但是不一定安全。 
3.  功能没有关系型数据库完善。 
4.  复杂表关联查询不容易实现。

## Redis的数据结构

Redis支持五种数据类型：string（字符串），[hash](https://so.csdn.net/so/search?q=hash&spm=1001.2101.3001.7020)（哈希），list（列表），set（无序集合）及zset(有序集合)。

![img](http://myblog.over2022.top/ea5cd0a2c3188ac466168386510a7a64.png) 

String：一个 key 对应一个字符串，string是Redis 最基本的数据类型。（字节的abase框架只实现了redis的string数据结构，导致我们如果想要存储复杂的数据结构的时候，只能转成json格式的字符串来存储）
list：一个 key 对应一个字符串列表，底层使用双向链表实现，很多双向链表支持的操作它都支持。

![img](http://myblog.over2022.top/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80OTMzNzAxLWJiYzAwMGIyZmY3NTVhYjkucG5n) 

Hash：
Set：比如一个Set的实例：A = {'a', 'b', 'c'}，A是集合的key，‘a’, 'b’和‘c’是集合的member。无序、无重复元素。
SortedSet：在set的基础上加上一个分数score，set里面的数据是有序的。

## Redis的持久化方式

**redis是基于内存的数据库**。

优点是cpu读取内存速度快，一秒钟可以进行数十万次，可以直接和cpu速度相近，读取极快。缺点是基于内存，存在断电数据丢失的情况。为了防止其数据断电丢失，就需要将数据存入硬盘中，这样在断电后也可以访问到数据库当中的数据。这个将内存的数据写入到磁盘中，防止服务器宕机内存数据丢失，就是redis的持久化。

redis提供两种持久化机制：[RDB](https://so.csdn.net/so/search?q=RDB&spm=1001.2101.3001.7020)和AOF	redis的**默认**持久化方式是RDB。

### 1、RDB（快照）

RDB：是Redis DataBase缩写。

按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为dump.rdb。

### 2、AOF（追加日志文件）
AOF：是Append Only File的缩写。

是指，将redis执行的所有写命令记录到日志文件中，将被执行的写命令写到AOF的文件末尾，当redis重启时，redis会从头到尾执行一次AOF文件所包含的所有写命令，以此恢复AOF文件的记录的数据集。

## Redis某节点短暂的卡顿原因

 过多的内存使用: Redis是内存数据库，如果内存使用过多，可能导致短暂的卡顿。

过多的读写请求: 读写请求过多会导致Redis节点忙碌，从而导致短暂的卡顿。

网络问题: 如果网络中存在瓶颈，读写请求的数据传输可能会出现延迟，从而导致短暂的卡顿。

新节点的加入，产生短暂的抖动

## Redis集群的演进

几乎所有系统都是在业务不断推进中演进的，包括淘宝、京东等，我们使用的组件自身也在不断的升级和完善，Redis亦是如此，为了支持更多的场景，更好的性能和吞吐等，其架构也在不断演进：

1. 单机；
2. 主从；
3. 高可用；
4. 集群；

### 单机模式

![img](https://pic1.zhimg.com/80/v2-968c9a85409dfd1fc5f04037f883e8d8_720w.webp)

单机模式是客户端直连一个redis实例即可，有以下优缺点：

优点：

- 简单（架构简单，部署简单）；
- 方便（开发方便，直接连上redis-server就行）；

缺点：

- 单点问题；
- 性能瓶颈；
- 可靠性弱；

### 主从模式

在分布式系统中为了解决单点问题，通常会把数据复制多个Replication（称为 ‘副本’）部署到其他机器，满足故障恢复和负载均衡等需求。Redis亦是如此，它为我们提供了复制功能，实现了相同数据的多个Redis副本。

Redis的复制模式结构可以支持单层或多层复制关系，分为以下三种：

- 一主一从
- 一主多从
- 树状主从

**一主一从结构**

![img](https://pic2.zhimg.com/80/v2-06a2da4c7e235d12a62ab8fe7f0f3e79_720w.webp)

这种结构是最简单的复制拓扑结构，用于主节点出现宕机时从节点提供故障转移支持。当应用写命令并发量较高且需要持久化时，可以只在从节点上开启AOF，这样既保证数据安全性同时也避免了持久化对主节点的性能干扰。但是需要注意的是，当主节点关闭持久化功能时，如果主节点脱机要避免自动重启操作。因为主节点之前没有开启持久化功能自动重启后数据集为空，这时从节点如果继续复制主节点会导致从节点数据也被清空的情况，丧失了持久化的意义。安全的做法是在从节点上执行 slaveof no one 断开与主节点的复制关系，再重启主节点从而避免这一问题。

**一主多从结构**

![img](https://pic1.zhimg.com/80/v2-e97b14f071834da45ca7fdad565cbcf8_720w.webp)

一主多从结构（又称为星形拓扑结构）使得应用端可以利用多个从节点实现读写分离。对于读占比较大的场景，可以把读命令发送到从节点来分担主节点的压力。同时在日常开发中如果需要执行一些比较耗时的读命令，如：keys、sort等，可以在其中一台从节点上执行，防止慢查询对主节点造成阻塞从而影响线上服务的稳定性。对于写并发量较高的场景，多个从节点会导致主节点写命令的多次发送从而过度消耗网络带宽，同时也加重了主节点的负载，影响服务的稳定性。

**树状主从结构**

![img](https://pic1.zhimg.com/80/v2-e46020300c0a6fc374b36f67b901b058_720w.webp)

这种结构使得从节点不但可以复制主节点数据，同时可以作为其他从节点的主节点继续向下层复制。通过引入复制中间层，可以有效降低主节点负载和需要传送给从节点的数据量。当主节点需要挂载多个从节点时为了避免对主节点的性能干扰，可以采用树状结构降低主节点压力。

优点：

- 高可用
- 读写分离

缺点：

- 故障恢复需要人工接入
- 主节点的单机风险

### 高可用模式

![img](https://pic3.zhimg.com/80/v2-959de4f09232f068a0b80b70d331b43a_720w.webp)

Redis主从复制模式下，一旦主节点出现了故障，需要人工干预进行故障转移，无论对于Redis的应用方还是运维方都带来了很大的不便。

Redis Sentinel是一个分布式机构，其中包含若干个Sentinel节点和Redis数据节点，每个Sentinel节点会对数据节点和其余Sentinel节点进行监控，当它发现节点不可达时，会对节点做下线标识。如果被标识的是主节点，它还会和其他Sentinel节点进行‘协商’，当大多数Sentinel节点都认为主节点不可达时，它们会选举一个Sentinel节点来完成自动故障转移的工作，同时会将这个变化实时通知给Redis应用方。整个过程都是自动的，不需要人工来介入。

优点：

- Redis Sentinel 解决 Redis 主从模式下的高可用问题

缺点：

- 不能解决负载均衡问题

### 集群模式

![img](https://pic4.zhimg.com/80/v2-c10f3fa8834c8d1c1bc49247b2b5d863_720w.webp)

Redis Cluster 是Redis的分布式解决方案，在3.0版本正式推出，有效地解决了Redis分布式的需求。当遇到单机内存、并发、流量等瓶颈时，可以采用Cluster架构方案达到负载均衡的目的。

优点：

- 无中心架构。
- 数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。
- 可扩展性：可线性扩展到 1000 多个节点，节点可动态添加或删除。
- 高可用性：部分节点不可用时，集群仍可用。通过增加 Slave 做 standby 数据副本，能够实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave 到 Master 的角色提升。
- 降低运维成本，提高系统的扩展性和可用性。

缺点：

- Key 批量操作限制，如使用 mset、mget 目前只支持具有相同 slot 值的 Key 执行批量操作。对于映射为不同 slot 值的 Key 由于 Keys 不支持跨 slot 查询，所以执行 mset、mget、sunion 等操作支持不友好。
- Key 事务操作支持有限，只支持多 key 在同一节点上的事务操作，当多个 Key 分布于不同的节点上时无法使用事务功能。
- Key 作为数据分区的最小粒度，不能将一个很大的键值对象如 hash、list 等映射到不同的节点。
- 其他
